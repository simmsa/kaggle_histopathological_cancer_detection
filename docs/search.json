[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "",
    "text": "The Histopathologic Cancer Detection Kaggle competition developed by Cukierski (2018) details a challenge in medical diagnostics: identifying metastatic cancer in small image patches extracted from larger digital pathology scans. This binary classification problem involves determining whether each image patch contains cancerous or non-cancerous tissue. The competition utilizes a modified version of the PatchCamelyon (PCam) dataset, a well-established benchmark in medical imaging, providing labeled image patches that facilitate the development and evaluation of image classification models.\nThis report aims to quantify the impact of architectural configurations and preprocessing strategies on model accuracy, employing common performance metrics and visualization techniques to analyze results. Ultimately, this work seeks to understand the application of CNNs in pathology image analysis and their potential contributions to automated cancer detection.\n\n\nAt a high level, this project aims to train, validate, and test different CNN architectures, as introduced in the Week 3 lectures:\n\nVGNet, defined in Listing 1, based on Very Deep Convolutional Networks for Large-Scale Image Recognition (Simonyan and Zisserman 2015)\n\nInceptionNet, defined in Listing 2, based on Going Deeper With Convolutions (Szegedy et al. 2014)\n\nResNet, defined in Listing 3, based on Deep Residual Learning for Image Recognition (He et al. 2015)\n\nThe training methodology will follow an iterative procedure: defining models, training them on labeled image patches, evaluating their performance on test data, and refining them through hyperparameter optimization. The cycle repeats to improve performance, offering a systematic approach to model development.\n\n\n\n\n\n\nflowchart LR\n    Define[\"Define Model\"] --&gt; Train[\"Train\"] --&gt; Validate --&gt;  TuneHyper[Optimize Hyperparameters] --&gt; Train --&gt; Final[\"Final Model\"] --&gt; Classify[Classify\\nTest Data] --&gt; Submit\n\n\n\n\nFigure 1: High Level Project Flowchart\n\n\n\n\n\nThis cyclical workflow defines a structured exploration of CNN performance, with a particular emphasis on targeted hyperparameter optimization of initial models. Given the extensive range of hyperparameters that can be adjusted, which could potentially lead to an ongoing and open-ended process, we will focus on a carefully selected subset of hyperparameters. These will be identified based on insights gained from exploratory data analysis conducted prior to bulk model training. This focused approach not only aims to develop high-performing models but also to uncover valuable insights into the complex relationships between model architecture, data characteristics, and predictive outcomes.\n\n\n\nThere are 3 data products necessary for training a CNN, training images, training classifications, and test images to classify. In the sections below we detail each of these distinct datasets.\n\n\nCode\nfrom pathlib import Path\nimport seaborn as sns\nsns.set_theme()\n\ntrain_img_path = Path(\"../../data/cancer_detection/histopathologic-cancer-detection/train\")\ntest_img_path = Path(\"../../data/cancer_detection/histopathologic-cancer-detection/test\")\n\n\n\n\n\n\n\nTable 1: Training Data Statistics\n\n\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nFormat\n96 x 96 px .tif images in train folder\n\n\nValue Count\n220025\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\n\n\ndf = pd.read_csv(\"../../data/cancer_detection/histopathologic-cancer-detection/train_labels.csv\")\n\n\n\n\n\nTable 2: Training Labels Statistics\n\n\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nFormat\nTabular Data in train_labels.csv\n\n\nValue Count\n220025\n\n\nColumns\n[‘id’, ‘label’]\n\n\nNaN Count\n0\n\n\nUnique Values\n0, 1\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3: Testing Data Statistics\n\n\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nFormat\n96 x 96 px .tif images in test folder\n\n\nValue Count\n57458\n\n\n\n\n\n\n\n\n\n\nThe Kaggle submission expects an output csv file called submission.csv with the columns:\n\n\n\nTable 4: Kaggle Submission Data Description\n\n\n\n\n\nColumn\nContent\n\n\n\n\nid\nInput image name without .tif\n\n\nlabel\nClassification, 0 for no cancer, 1 for cancer"
  },
  {
    "objectID": "index.html#approach-methodology",
    "href": "index.html#approach-methodology",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "",
    "text": "At a high level, this project aims to train, validate, and test different CNN architectures, as introduced in the Week 3 lectures:\n\nVGNet, defined in Listing 1, based on Very Deep Convolutional Networks for Large-Scale Image Recognition (Simonyan and Zisserman 2015)\n\nInceptionNet, defined in Listing 2, based on Going Deeper With Convolutions (Szegedy et al. 2014)\n\nResNet, defined in Listing 3, based on Deep Residual Learning for Image Recognition (He et al. 2015)\n\nThe training methodology will follow an iterative procedure: defining models, training them on labeled image patches, evaluating their performance on test data, and refining them through hyperparameter optimization. The cycle repeats to improve performance, offering a systematic approach to model development.\n\n\n\n\n\n\nflowchart LR\n    Define[\"Define Model\"] --&gt; Train[\"Train\"] --&gt; Validate --&gt;  TuneHyper[Optimize Hyperparameters] --&gt; Train --&gt; Final[\"Final Model\"] --&gt; Classify[Classify\\nTest Data] --&gt; Submit\n\n\n\n\nFigure 1: High Level Project Flowchart\n\n\n\n\n\nThis cyclical workflow defines a structured exploration of CNN performance, with a particular emphasis on targeted hyperparameter optimization of initial models. Given the extensive range of hyperparameters that can be adjusted, which could potentially lead to an ongoing and open-ended process, we will focus on a carefully selected subset of hyperparameters. These will be identified based on insights gained from exploratory data analysis conducted prior to bulk model training. This focused approach not only aims to develop high-performing models but also to uncover valuable insights into the complex relationships between model architecture, data characteristics, and predictive outcomes."
  },
  {
    "objectID": "index.html#data-descriptions",
    "href": "index.html#data-descriptions",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "",
    "text": "There are 3 data products necessary for training a CNN, training images, training classifications, and test images to classify. In the sections below we detail each of these distinct datasets.\n\n\nCode\nfrom pathlib import Path\nimport seaborn as sns\nsns.set_theme()\n\ntrain_img_path = Path(\"../../data/cancer_detection/histopathologic-cancer-detection/train\")\ntest_img_path = Path(\"../../data/cancer_detection/histopathologic-cancer-detection/test\")\n\n\n\n\n\n\n\nTable 1: Training Data Statistics\n\n\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nFormat\n96 x 96 px .tif images in train folder\n\n\nValue Count\n220025\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\n\n\ndf = pd.read_csv(\"../../data/cancer_detection/histopathologic-cancer-detection/train_labels.csv\")\n\n\n\n\n\nTable 2: Training Labels Statistics\n\n\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nFormat\nTabular Data in train_labels.csv\n\n\nValue Count\n220025\n\n\nColumns\n[‘id’, ‘label’]\n\n\nNaN Count\n0\n\n\nUnique Values\n0, 1\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3: Testing Data Statistics\n\n\n\n\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nFormat\n96 x 96 px .tif images in test folder\n\n\nValue Count\n57458"
  },
  {
    "objectID": "index.html#expected-output",
    "href": "index.html#expected-output",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "",
    "text": "The Kaggle submission expects an output csv file called submission.csv with the columns:\n\n\n\nTable 4: Kaggle Submission Data Description\n\n\n\n\n\nColumn\nContent\n\n\n\n\nid\nInput image name without .tif\n\n\nlabel\nClassification, 0 for no cancer, 1 for cancer"
  },
  {
    "objectID": "index.html#training-data-classifications",
    "href": "index.html#training-data-classifications",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "2.1 Training Data Classifications",
    "text": "2.1 Training Data Classifications\nTraining data is provided in two forms, .tif images and a csv file mapping .tif images to labels. Parsing train_labels.csv yield specifications about the training dataset:\n\n\nCode\nfrom IPython.display import Markdown, display\n\n\ndisplay(Markdown(df.head().to_markdown(index=False)))\n\n\n\n\nTable 5: Training Data Labels\n\n\n\n\n\n\nid\nlabel\n\n\n\n\nf38a6374c348f90b587e046aac6079959adf3835\n0\n\n\nc18f2d887b7ae4f6742ee445113fa1aef383ed77\n1\n\n\n755db6279dae599ebb4d39a9123cce439965282d\n0\n\n\nbc3f0c64fb968ff4a8bd33af6971ecae77c75e08\n0\n\n\n068aba587a4950175d04c680d38943fd488d6a9d\n0\n\n\n\n\n\n\n\n\nIn Table 5 the column id maps to a .tif file in the train folder and the label indicates if the images is classified as having cancer, 1 or not having cancer, 0\n\n2.1.1 Training Label Statistics\n\n\n\nTable 6: Training Label Data Statistics\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nValue Count\n220025\n\n\nNaN Count\n0\n\n\nUnique Values\n0, 1\n\n\nPercentage of 1\n40.5031%\n\n\nPercentage of 0\n59.4969%\n\n\n\n\n\n\nIn Table 6 there we calculate that roughly 40 percent of the training data has a training label of 1. While not entirely unexpected, this is interesting and may be useful as a secondary validation check during the training process"
  },
  {
    "objectID": "index.html#training-images",
    "href": "index.html#training-images",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "2.2 Training Images",
    "text": "2.2 Training Images\nIn Figure 2 the first 5 images in the dataset are visualized at their full resolution. Per the Kaggle data description the images are 96x96px, but the classification was done within the 32x32px section.\n\nCode\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n\nimg_labels = df['id'].iloc[:5].to_list()\n\ndef plot_cancer_images(img_labels, zoom=False, scale_factor=1):\n    for img in img_labels:\n        img_path = Path(train_img_path, f\"{img}.tif\")\n        with Image.open(img_path) as img:\n            if zoom:\n                # Crop to the center 32x32 pixels\n                width, height = img.size\n                left = (width - 32) // 2\n                top = (height - 32) // 2\n                right = left + 32\n                bottom = top + 32\n                img = img.crop((left, top, right, bottom))\n            \n            # Get the new image size\n            width, height = img.size\n            \n            # Apply scale factor if zoomed\n            width *= scale_factor\n            height *= scale_factor\n            \n            # Set figure size to match the scaled image size\n            dpi = 100  # Dots per inch\n            figsize = (width / dpi, height / dpi)\n            \n            # Create figure and axes\n            fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n            ax.imshow(img)\n            ax.axis(\"off\")  # Turn off axes\n            \n            # Remove all margins and display inline\n            plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n            plt.show()\n\nplot_cancer_images(img_labels)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 2: Sample of Training Images\n\n\n\n\n2.2.1 Full Size Image Comparison\nA comparison of full size positive and negatives is shown in Figure 3 and Figure 4. The purpose of this visualization is to determine if there are obvious visual differences between the two image classes.\n\n\n\n\n2.2.1.1 Positive Sample Images\n\nCode\npositive_labels = df.loc[df['label'] == 1, 'id'].iloc[:25].to_list()\nplot_cancer_images(positive_labels)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\n\n\n\n\n\n(j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(k)\n\n\n\n\n\n\n\n\n\n\n\n(l)\n\n\n\n\n\n\n\n\n\n\n\n(m)\n\n\n\n\n\n\n\n\n\n\n\n(n)\n\n\n\n\n\n\n\n\n\n\n\n(o)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(p)\n\n\n\n\n\n\n\n\n\n\n\n(q)\n\n\n\n\n\n\n\n\n\n\n\n(r)\n\n\n\n\n\n\n\n\n\n\n\n(s)\n\n\n\n\n\n\n\n\n\n\n\n(t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(u)\n\n\n\n\n\n\n\n\n\n\n\n(v)\n\n\n\n\n\n\n\n\n\n\n\n(w)\n\n\n\n\n\n\n\n\n\n\n\n(x)\n\n\n\n\n\n\n\n\n\n\n\n(y)\n\n\n\n\n\n\n\nFigure 3: Sample of Positive Training Images\n\n\n\n\n\n\n\n\n2.2.1.2 Negative Sample Images\n\nCode\nnegative_labels = df.loc[df['label'] == 0, 'id'].iloc[:25].to_list()\nplot_cancer_images(negative_labels)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\n\n\n\n\n\n(j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(k)\n\n\n\n\n\n\n\n\n\n\n\n(l)\n\n\n\n\n\n\n\n\n\n\n\n(m)\n\n\n\n\n\n\n\n\n\n\n\n(n)\n\n\n\n\n\n\n\n\n\n\n\n(o)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(p)\n\n\n\n\n\n\n\n\n\n\n\n(q)\n\n\n\n\n\n\n\n\n\n\n\n(r)\n\n\n\n\n\n\n\n\n\n\n\n(s)\n\n\n\n\n\n\n\n\n\n\n\n(t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(u)\n\n\n\n\n\n\n\n\n\n\n\n(v)\n\n\n\n\n\n\n\n\n\n\n\n(w)\n\n\n\n\n\n\n\n\n\n\n\n(x)\n\n\n\n\n\n\n\n\n\n\n\n(y)\n\n\n\n\n\n\n\nFigure 4: Sample of Negative Training Images\n\n\n\n\n\n\n\nIn Figure 3 and Figure 4 we observe that both image classes appear to have similar color characteristics with the color pallet leaning towards red/purple. Additionally, some of the negative training images have large areas of white, but this could be due to chance. Overall, both classes appear to be candidates for identification using CNN techniques.\n\n\n2.2.2 Zoomed Image Comparison\nOf additional interest is a visual inspection of the images cropped to the cancer detection area of 32 x 32 px. All positive and negative samples are the same as above.\n\n\n\n\n2.2.2.1 Zoomed Positive Sample Images\n\nCode\npositive_labels = df.loc[df['label'] == 1, 'id'].iloc[:25].to_list()\nplot_cancer_images(positive_labels, zoom=True, scale_factor=3)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\n\n\n\n\n\n(j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(k)\n\n\n\n\n\n\n\n\n\n\n\n(l)\n\n\n\n\n\n\n\n\n\n\n\n(m)\n\n\n\n\n\n\n\n\n\n\n\n(n)\n\n\n\n\n\n\n\n\n\n\n\n(o)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(p)\n\n\n\n\n\n\n\n\n\n\n\n(q)\n\n\n\n\n\n\n\n\n\n\n\n(r)\n\n\n\n\n\n\n\n\n\n\n\n(s)\n\n\n\n\n\n\n\n\n\n\n\n(t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(u)\n\n\n\n\n\n\n\n\n\n\n\n(v)\n\n\n\n\n\n\n\n\n\n\n\n(w)\n\n\n\n\n\n\n\n\n\n\n\n(x)\n\n\n\n\n\n\n\n\n\n\n\n(y)\n\n\n\n\n\n\n\nFigure 5: 32 x 32px Sample of Positive Training Images\n\n\n\n\n\n\n\n\n2.2.2.2 Zoomed Negative Sample Images\n\nCode\nnegative_labels = df.loc[df['label'] == 0, 'id'].iloc[:25].to_list()\nplot_cancer_images(negative_labels, zoom=True, scale_factor=3)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\n\n\n\n\n\n(j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(k)\n\n\n\n\n\n\n\n\n\n\n\n(l)\n\n\n\n\n\n\n\n\n\n\n\n(m)\n\n\n\n\n\n\n\n\n\n\n\n(n)\n\n\n\n\n\n\n\n\n\n\n\n(o)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(p)\n\n\n\n\n\n\n\n\n\n\n\n(q)\n\n\n\n\n\n\n\n\n\n\n\n(r)\n\n\n\n\n\n\n\n\n\n\n\n(s)\n\n\n\n\n\n\n\n\n\n\n\n(t)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(u)\n\n\n\n\n\n\n\n\n\n\n\n(v)\n\n\n\n\n\n\n\n\n\n\n\n(w)\n\n\n\n\n\n\n\n\n\n\n\n(x)\n\n\n\n\n\n\n\n\n\n\n\n(y)\n\n\n\n\n\n\n\nFigure 6: 32 x 32px Sample of Negative Training Images\n\n\n\n\n\n\n\nIn Figure 5 and Figure 6 we again observe no obvious differences between the two classes. Some of the negative samples are predominately white, which should be easily classified by the CNN models."
  },
  {
    "objectID": "index.html#vgnet-pytorch-definition",
    "href": "index.html#vgnet-pytorch-definition",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "3.1 VGNet PyTorch Definition",
    "text": "3.1 VGNet PyTorch Definition\nVGNet as described in Very Deep Convolutional Networks for Large-Scale Image Recognition (Simonyan and Zisserman 2015), is a deep CNN designed for large-scale image classification tasks. The architecture consists of a series of convolutional layers followed by fully connected layers. The key feature of VGNet is its depth, with a large number of layers contributing to its high capacity for learning from complex data.\nIn Listing 1, we define the layers in the following order:\n\nThree convolutional blocks with increasing numbers of filters.\nMaxPooling layers to reduce spatial dimensions.\nFully connected layers to classify the output features.\n\nThe network is designed to take an input image and output a prediction for one of the predefined classes. The code provided below is a direct translation of the VGNet architecture to PyTorch.\n\n\n\n\n\nListing 1: VGNet PyTorch CNN Model\n\n\nclass VGNet(nn.Module):\n    def __init__(self, input_size, num_classes=2):\n        super(VGNet, self).__init__()\n        # Define 3 convolution layers\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(2, 2)\n\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(2, 2)\n\n        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(2, 2)\n\n        self.input_size = input_size\n        self.feature_map_size = self._get_feature_map_size(input_size)\n\n        self.fc1 = nn.Linear(self.feature_map_size, 1024)\n        self.fc2 = nn.Linear(1024, 1024)\n        # Binary Classification\n        self.fc3 = nn.Linear(1024, num_classes)\n\n    # Handle different size images\n    def _get_feature_map_size(self, input_size):\n        size = input_size // 2  # After pool1\n        size = size // 2  # After pool2\n        size = size // 2  # After pool3\n        return 256 * size * size  # 256 channels in the last conv layer\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = self.pool1(x)\n\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = self.pool2(x)\n\n        x = F.relu(self.conv5(x))\n        x = F.relu(self.conv6(x))\n        x = self.pool3(x)\n\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
  },
  {
    "objectID": "index.html#inceptionnet-pytorch-definition",
    "href": "index.html#inceptionnet-pytorch-definition",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "3.2 InceptionNet PyTorch Definition",
    "text": "3.2 InceptionNet PyTorch Definition\nThis InceptionNet model is based on Going Deeper With Convolutions (Szegedy et al. 2014). InceptionNet introduces the idea of using multiple filter sizes in parallel within the same layer, which allows the model to capture different types of features. The architecture includes various branches, each performing a different convolution operation, and these are concatenated to form the output.\nIn Listing 2 we define the InceptionBlock, which contains these multiple branches, and then stack two such blocks in sequence. The network also uses a large initial convolution layer and pooling layers for spatial dimension reduction. The final output is classified through a fully connected layer.\n\n\n\n\n\nListing 2: InceptionNet PyTorch CNN Model\n\n\nclass InceptionBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(InceptionBlock, self).__init__()\n        self.branch1x1 = nn.Conv2d(in_channels, 64, kernel_size=1)\n\n        self.branch3x3 = nn.Sequential(\n            nn.Conv2d(in_channels, 64, kernel_size=1),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n        )\n\n        self.branch5x5 = nn.Sequential(\n            nn.Conv2d(in_channels, 32, kernel_size=1),\n            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n        )\n\n        self.branch_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(in_channels, 64, kernel_size=1),\n        )\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n        branch3x3 = self.branch3x3(x)\n        branch5x5 = self.branch5x5(x)\n        branch_pool = self.branch_pool(x)\n        return torch.cat([branch1x1, branch3x3, branch5x5, branch_pool], dim=1)\n\n\nclass InceptionNetLike(nn.Module):\n    def __init__(self, input_size, num_classes=2):\n        super(InceptionNetLike, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n        self.pool1 = nn.MaxPool2d(3, 2, padding=1)\n\n        self.inception1 = InceptionBlock(64)\n        self.inception2 = InceptionBlock(320)\n        self.pool2 = nn.MaxPool2d(3, 2, padding=1)\n\n        # Dynamically compute the flattened size for the fully connected layer\n        self.input_size = input_size\n        self.feature_map_size = self._get_feature_map_size(input_size)\n        self.fc = nn.Linear(self.feature_map_size, num_classes)\n\n\n    # Handle different sized inputs\n    def _get_feature_map_size(self, input_size):\n        size = (input_size + 2 * 3 - 7) // 2 + 1  # After conv1\n        size = (size + 2 * 1 - 3) // 2 + 1  # After pool1\n        size = (size + 2 * 1 - 3) // 2 + 1  # After pool2\n        return 320 * size * size  # 320 channels in the last InceptionBlock\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool1(x)\n\n        x = self.inception1(x)\n        x = self.inception2(x)\n        x = self.pool2(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x"
  },
  {
    "objectID": "index.html#resnet-pytorch-definition",
    "href": "index.html#resnet-pytorch-definition",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "3.3 ResNet PyTorch Definition",
    "text": "3.3 ResNet PyTorch Definition\nResNet introduced in Deep Residual Learning for Image Recognition (He et al. 2015), is known for its innovative use of residual connections, which help mitigate the problem of vanishing gradients in deep networks. These connections allow the network to learn residual functions instead of direct mappings, significantly improving the training of very deep networks.\nThe following PyTorch implementation follows the ResNet-like architecture with three layers, each containing multiple residual blocks. The model utilizes batch normalization and convolutional layers to progressively extract features before classifying them with a fully connected layer.\n\n\n\n\n\nListing 3: ResNet PyTorch CNN Model\n\n\nclass ResNetLike(nn.Module):\n    def __init__(self, input_size, num_classes=2):\n        super(ResNetLike, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        self.layer1 = self._make_layer(64, 64, stride=1)\n        self.layer2 = self._make_layer(64, 128, stride=2)\n        self.layer3 = self._make_layer(128, 256, stride=2)\n\n        # Hard code feature map output size\n        if input_size == 32:\n            # 16384\n            self.feature_map_size = 32 * 512\n        elif input_size == 48:\n            # 36864\n            self.feature_map_size = 48 * 768\n        else:\n            # 1478956\n            self.feature_map_size = 96 * 1536\n\n        self.fc = nn.Linear(self.feature_map_size, num_classes)\n\n    def _make_layer(self, in_channels, out_channels, stride):\n        return nn.Sequential(\n            ResidualBlock(in_channels, out_channels, stride),\n            ResidualBlock(out_channels, out_channels, 1),\n        )\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x"
  },
  {
    "objectID": "index.html#data-loader",
    "href": "index.html#data-loader",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "4.1 Data Loader",
    "text": "4.1 Data Loader\nUsing PyTorch’s DataLoader, the input is split into training and validation subsets with batching and shuffling enabled. The CancerDataset class handles loading and preprocessing image data. Transformations are specified as global variables and passed as input into the CancerDataset class.\n\n\n\n\nListing 4: Loading Data into Model\n\n\nclass CancerDataset(Dataset):\n    def __init__(self, dataframe, img_folder, transform=None):\n        self.dataframe = dataframe\n        self.img_folder = img_folder\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_id = self.dataframe.iloc[idx, 0]\n        label = self.dataframe.iloc[idx, 1]\n        img_path = self.img_folder / f\"{img_id}.tif\"\n        image = Image.open(img_path)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\nif TRANSFORM_TYPE == \"NONE\":\n    transform = transforms.Compose([transforms.ToTensor()])\nelif TRANSFORM_TYPE == \"48PX_CROP\":\n    transform = transforms.Compose([transforms.ToTensor(), transforms.CenterCrop(48)])\nelif TRANSFORM_TYPE == \"32PX_CROP\":\n    transform = transforms.Compose([transforms.ToTensor(), transforms.CenterCrop(32)])\nelse:\n    raise ValueError(f\"Unexpected TRANSFORM_TYPE {TRANSFORM_TYPE}\")\n\n\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=1234)\ntrain_dataset = CancerDataset(train_df, train_img_folder, transform)\nval_dataset = CancerDataset(val_df, train_img_folder, transform=transform)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True\n)"
  },
  {
    "objectID": "index.html#training-loop",
    "href": "index.html#training-loop",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "4.2 Training Loop",
    "text": "4.2 Training Loop\nThe training loop uses a cross-entropy loss function and the Adam optimizer to adjust the model’s parameters. During each epoch, batches of data are passed through the model, and the loss is computed and minimized using backpropagation. The loop is designed to iteratively improve the model’s ability to predict class labels accurately.\n\n\n\n\nListing 5: Training Loop\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(100):\n    model.train()\n\n    running_loss = 0.0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        labels = labels.float()\n        optimizer.zero_grad()\n        outputs = model(images)\n\n        loss = criterion(outputs.squeeze(), labels)\n\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()"
  },
  {
    "objectID": "index.html#hyperparameter-determination",
    "href": "index.html#hyperparameter-determination",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "4.3 Hyperparameter Determination",
    "text": "4.3 Hyperparameter Determination\nThe training process begins with three predefined model architectures, serving as the foundation for hyperparameter exploration. The key focus is to identify static (unchanging) and dynamic (tunable) hyperparameters to optimize model performance. By concentrating on high-impact hyperparameters, the project narrows its scope to a manageable set of experiments, balancing the need for thorough exploration with practical constraints. This methodical approach prevents an unbounded exploration of the hyperparameter space while still ensuring robust optimization of the selected models.\n\n4.3.1 Batch Size\nbatch_size is an integer passed to torch.utils.DataLoader, specifies the number of samples loaded per batch. During initial experiments, batch size showed a clear correlation with both model execution time and memory usage, making it a critical hyperparameter to tune.\nTo determine the optimal batch size, multiple experiments were conducted using a single model architecture while varying the batch size. The results revealed a tradeoff between execution time and batch size.\n\n\n\nCode\nbatch_model_path = Path(\"../model_metrics_output/\").resolve()\nbatch_model_files = [\n    Path(batch_model_path, \"v2.epochs_1.batch_size_8.parquet\"),\n    Path(batch_model_path, \"v2.epochs_1.batch_size_16.parquet\"),\n    Path(batch_model_path, \"v2.epochs_1.batch_size_32.parquet\"),\n    Path(batch_model_path, \"v2.epochs_5.batch_size_64.parquet\"),\n    Path(batch_model_path, \"v2.epochs_1.batch_size_128.parquet\"),\n    Path(batch_model_path, \"v2.epochs_1.batch_size_256.parquet\"),\n    Path(batch_model_path, \"v2.epochs_1.batch_size_512.parquet\"),\n]\n\nbatch_df = [pd.read_parquet(f) for f in batch_model_files]\n\nbatch_df[0].head()\n\nmean_times = []\nbatch_sizes = []\n\nfor df, file_path in zip(batch_df, batch_model_files):\n    name = file_path.name\n    parts = name.split(\".\")\n    batch_size = int(parts[2].split(\"_\")[-1])\n    mean_execution_time = df[\"execution_time\"].mean()\n\n    batch_sizes.append(batch_size)\n    mean_times.append(mean_execution_time)\n\nbatch_time_df = pd.DataFrame({\"Mean Execution Time [s]\": mean_times}, index=batch_sizes)\nbatch_time_df.index.name = \"Batch Size\"\n\nbatch_time_df.plot(\n    kind=\"bar\",\n    figsize=(8, 2.5),\n    # linewidth=0.5,\n    # marker=\".\",\n    # markersize=1.5,\n    ylabel=\"Execution Time [s]\",\n    legend=False,\n    rot=0,\n)\nplt.ylim(0, 400)\n\n\n\n\n\n\n\n\nFigure 7: Execution Time vs. Batch Size\n\n\n\n\n\n\n\nCode\nbatch_time_df = batch_time_df.reset_index()\nbatch_time_df = batch_time_df.round(2)\nbatch_time_df\n\n\n\n\nTable 7: Mean Execution Time\n\n\n\n\n\n\n\n\n\n\nBatch Size\nMean Execution Time [s]\n\n\n\n\n0\n8\n390.50\n\n\n1\n16\n316.26\n\n\n2\n32\n255.17\n\n\n3\n64\n236.77\n\n\n4\n128\n221.08\n\n\n5\n256\n220.11\n\n\n6\n512\n232.28\n\n\n\n\n\n\n\n\n\n\n\nFrom the batch size exploration detailed in Figure 7 and Table 7, it was observed that a batch size of 256 provided an optimal balance between execution time and resource usage for this model architecture. This batch size will be set at 256 for all training runs, enabling the most efficient use of time.\n\n\n4.3.2 Image Size\nDuring the early stages of testing, the effect of image size on model performance was uncovered through unexpected observations. Initial experiments omitted cropping during testing, leading to higher test accuracy. However, when a cropping strategy was later applied to validation data, validation accuracy decreased by approximately 15%. This discrepancy highlighted the importance of exploring the relationship between image size and model performance.\nIn experimenting with various cropping strategies, we observed that altering cropping dimensions significantly influenced model accuracy. Specifically, cropping sizes of 32 pixels, 48 pixels, and no cropping were identified as key hyperparameters for tuning. These variations had a substantial impact on the model’s generalization capabilities, emphasizing the critical role of image preprocessing in the model pipeline.\nImage size does have an effect on the model execution time, which is detailed in Section 4.8.\n\n\n4.3.3 Other Potential Hyperparameters\nSeveral other potential dynamic hyperparameters were considered during the exploration phase, including:\n\nStride: Adjusting the step size during convolution operations.\nPadding: Modifying the boundary handling during convolutions.\nModel Layers and Complexity: Exploring deeper or more complex architectures.\nRandomizing Input Images: Introducing variability in training samples.\nNormalizing Input Images: Standardizing pixel values for faster convergence.\n\nWhile these parameters are likely influence model performance, they were ultimately kept static. This decision was motivated by the need to balance computational feasibility with meaningful experimentation, given the constraints of this project."
  },
  {
    "objectID": "index.html#training-specification",
    "href": "index.html#training-specification",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "4.4 Training Specification",
    "text": "4.4 Training Specification\nThis section specificions for the trained models. A total of nine models were trained, each evaluated over 100 epochs with a batch size of 256. The Adam optimizer was employed with a learning rate of 0.001, and the loss function was defined as cross-entropy, reflecting the binary classification nature of the task. Notably, no image normalization techniques were applied during preprocessing, while padding and stride were set to 1 to maintain the spatial dimensions of the input.\nThe models were designed using three distinct architecture types: VGNet, InceptionNet, and ResNet, to assess the impact of architectural variation on performance. Additionally, three cropping types (none, 48px center, and 32px center) were explored to understand how spatial input modifications influence learning. Tables Table 8 and Table 9 summarize the hyperparameters that remained consistent across experiments and those that varied between models, respectively.\n\n\n\nTable 8: Common Model Hyperparameters\n\n\n\n\n\nDescription\nValue\n\n\n\n\nModels Trained\n9\n\n\nNumber of Epochs\n100\n\n\nBatch Size\n256\n\n\nOptimizer\nAdam\n\n\nLearning Rate\n0.001\n\n\nLoss Function\nCross Entropy\n\n\nImage Normalization\nNone\n\n\nPadding\n1\n\n\nStride\n1\n\n\n\n\n\n\n\n\n\nTable 9: Changed Model Hyperparameters\n\n\n\n\n\nDescription\nValue\n\n\n\n\nModel Types\n3 (VGNet, InceptionNet, ResNet)\n\n\nCropping Types\n3 (None, 48px Center, 32px Center)"
  },
  {
    "objectID": "index.html#training-flowchart",
    "href": "index.html#training-flowchart",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "4.5 Training Flowchart",
    "text": "4.5 Training Flowchart\nVisualized in Figure 8, the training process begins with defining the CNN architecture and initializing key hyperparameters, such as model type and input image size. Training is conducted using labeled image patches, with the model learning to classify cancerous and non-cancerous regions. Testing evaluates the model’s accuracy and robustness, enabling the identification of strengths and weaknesses. Results are iteratively refined by tuning hyperparameters, retraining models, and optimizing their architectures.\n\n\n\n\n\n\nflowchart LR\n\n    subgraph TrainingData[Training Data]\n        OriginalData[\"Original Data\"]\n    end\n\n    subgraph Train\n        TrainImg[Training Images]\n        TrainLabels[Training Labels]\n    end\n\n    subgraph Validate\n        ValidateImg[Validation Images]\n        ValidateLabels[Validation Labels]\n    end\n\n    subgraph Model\n        CNN[Convolutional\\nNeural\\nNetwork]\n        subgraph HyperParameters\n            ModelType[Model Type]\n            ImageSize[Image Dimensions]\n        end\n        OptimizedCNN[\"Final Model\"]\n    end\n\n    subgraph Test\n        TestImg[Test Images]\n        TestLabels[Test Labels]\n    end\n\n    subgraph Tune\n        TuneHyper[Optimize Hyperparameters]\n    end\n\n    subgraph Kaggle\n        Submit[Submission]\n    end\n\nOriginalData --&gt;|\"80%\"| Train\nOriginalData --&gt;|\"20%\"| Validate\n\nTrainImg --&gt; CNN\nTrainLabels --&gt; CNN\n\nModelType --&gt; CNN\nImageSize --&gt; CNN\n\nValidateImg --&gt; CNN --&gt; ValidateLabels --&gt; TuneHyper --&gt; CNN\n\nTuneHyper --&gt; OptimizedCNN\nCNN --&gt; OptimizedCNN\nTestImg --&gt; OptimizedCNN --&gt; TestLabels --&gt; Submit\n\n\n\n\n\n\nFigure 8: Detailed Project Flowchart"
  },
  {
    "objectID": "index.html#training-hardware",
    "href": "index.html#training-hardware",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "4.6 Training Hardware",
    "text": "4.6 Training Hardware\nThis project utilizes an M2 MacBook Pro Max with 32GB of RAM, leveraging PyTorch’s Metal Performance Shaders backend for GPU acceleration. The Metal backend enables efficient training on Apple Silicon devices, harnessing the GPU to accelerate deep learning workloads. This setup was chosen as a stable alternative to the Kaggle platform and has adequate performance."
  },
  {
    "objectID": "index.html#training-procedure",
    "href": "index.html#training-procedure",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "4.7 Training Procedure",
    "text": "4.7 Training Procedure\nModels were trained sequentially in the order specified in Table 10. Model specifications are passed via command line arguments and each model run starts the script in a clean state. In total 9 model runs were performed with different combinations of model and crop hyperparameters. All other parameters specified in Table 8 were held constant for all training runs.\n\n\n\nTable 10: Training Order\n\n\n\n\n\n\n\n\n\n\nTraining Run #\nModel Hyperparameter\nCrop Specification Hyperparameter\n\n\n\n\n1\nVGNet\n32 x 32\n\n\n2\nInceptionNet\n32 x 32\n\n\n3\nResNet\n32 x 32\n\n\n4\nVGNet\n48 x 48\n\n\n5\nInceptionNet\n48 x 48\n\n\n6\nResNet\n48 x 48\n\n\n7\nVGNet\nFull Resolution\n\n\n8\nInceptionNet\nFull Resolution\n\n\n9\nResNet\nFull Resolution\n\n\n\n\n\n\nDuring each each epoch of each training run the following metrics were collected and saved.\n\n\n\nTable 11: List of Collected Model Metrics\n\n\n\n\n\nName\nUnit\n\n\n\n\nEpoch\nCount\n\n\nExecution Time\nSeconds\n\n\nTraining Loss\nAverage\n\n\nModel Specs\nString\n\n\n\n\n\n\nThese metrics will be used to determine the performance of each model"
  },
  {
    "objectID": "index.html#sec-model-exec-time",
    "href": "index.html#sec-model-exec-time",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "4.8 Model Execution Times",
    "text": "4.8 Model Execution Times\n\n\n\nCode\nimport seaborn as sns\nsns.set_theme()\n\nresults_path = Path(\"../model_metrics_output/\").resolve()\nmetrics = [\n    \"v3.model_type_VGNet.transform_32PX_CROP.padding_1.stride_1.epochs_spec_100.parquet\",\n    \"v3.model_type_InceptionNet.transform_32PX_CROP.padding_1.stride_1.epochs_spec_100.parquet\",\n    \"v3.model_type_ResNet.transform_32PX_CROP.padding_1.stride_1.epochs_spec_100.parquet\",\n    \"v3.model_type_VGNet.transform_48PX_CROP.padding_1.stride_1.epochs_spec_100.parquet\",\n    \"v3.model_type_InceptionNet.transform_48PX_CROP.padding_1.stride_1.epochs_spec_100.parquet\",\n    \"v3.model_type_ResNet.transform_48PX_CROP.padding_1.stride_1.epochs_spec_100.parquet\",\n    \"v3.model_type_VGNet.transform_NONE.padding_1.stride_1.epochs_spec_100.parquet\",\n    \"v3.model_type_InceptionNet.transform_NONE.padding_1.stride_1.epochs_spec_100.parquet\",\n    \"v3.model_type_ResNet.transform_NONE.padding_1.stride_1.epochs_spec_100.parquet\",\n]\n\n\nmetrics_df = [pd.read_parquet(Path(results_path, p)) for p in metrics]\n\nmetrics_combined = pd.concat(metrics_df, axis=\"index\")\n\n\n# Parse model type from the model column\nmetrics_combined['model_type'] = metrics_combined['model'].str.extract(r'model_type_([A-Za-z0-9]+)')\n\n# Group by model_type and aggregate the required metrics\nmetrics_grouped = metrics_combined.groupby('model').agg(\n    max_epoch=('epoch', 'max'),\n    mean_learning_rate=('learning_rate', 'mean'),\n    mean_execution_time=('execution_time', 'mean'),\n    max_execution_time=('execution_time', 'max'),\n    min_execution_time=('execution_time', 'min'),\n    total_execution_time=('execution_time', 'sum'),\n    mean_training_loss=('training_loss', 'mean'),\n    max_training_loss=('training_loss', 'max'),\n    min_training_loss=('training_loss', 'min')\n).reset_index()\n\n# Extract model_type and transform_type from the 'model' column\nmetrics_grouped['model_type'] = metrics_grouped['model'].str.extract(r'model_type_([A-Za-z0-9]+)')\nmetrics_grouped['transform_type'] = metrics_grouped['model'].str.extract(r'transform_([A-Za-z0-9_]+)')\n\n# metrics_grouped['transform_type' == \"NONE\"] = \"Full Resolution\"\n# metrics_grouped['transform_type' == \"48PX_CROP\"] = \"Center Crop - 48PX\"\n# metrics_grouped['transform_type' == \"32PX_CROP\"] = \"Center Crop - 32PX\"\n# metrics_grouped['training_time_seconds']\n\nmetrics_grouped.loc[metrics_grouped['transform_type'] == \"NONE\", 'transform_type'] = \"Full Resolution\"\nmetrics_grouped.loc[metrics_grouped['transform_type'] == \"48PX_CROP\", 'transform_type'] = \"Center Crop - 48PX\"\nmetrics_grouped.loc[metrics_grouped['transform_type'] == \"32PX_CROP\", 'transform_type'] = \"Center Crop - 32PX\"\n\n\nmetrics_grouped['total_execution_time_hours'] = metrics_grouped['total_execution_time'] / 60 / 60\n\nmetrics_grouped = metrics_grouped.sort_values(['total_execution_time'], ascending=False)\n# metrics_grouped[[\"model_type\", \"transform_type\", \"total_execution_time_hours\", \"mean_execution_time\", \"max_execution_time\",\n# \"min_execution_time\"]]\n\n# Calculate the error bars\nmetrics_grouped['error_min'] = metrics_grouped['mean_execution_time'] - metrics_grouped['min_execution_time']\nmetrics_grouped['error_max'] = metrics_grouped['max_execution_time'] - metrics_grouped['mean_execution_time']\n\n# Combine errors for yerr\nmetrics_grouped['yerr'] = list(zip(metrics_grouped['error_min'], metrics_grouped['error_max']))\n\n# Create the barplot\nplt.figure(figsize=(7.5, 3.5))\nbarplot = sns.barplot(\n    data=metrics_grouped,\n    x=\"model_type\",\n    y=\"mean_execution_time\",\n    hue=\"transform_type\",\n    errorbar=None\n)\n\n# Add error bars\nfor idx, bar in enumerate(barplot.patches[:-3]):\n    x = bar.get_x() + bar.get_width() / 2\n    y = bar.get_height()\n    error = metrics_grouped['yerr'][idx]\n    plt.errorbar(x, y, yerr=[[error[0]], [error[1]]], fmt='none', c='black', capsize=5)\n\nplt.ylabel(\"Execution Time Per Epoch [s]\")\nplt.xlabel(None)\nplt.legend(title=\"Transform Type\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Model Execution Time [seconds] per Training Epoch\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(7.5, 3.5))\nsns.barplot(metrics_grouped, x = \"model_type\", y=\"total_execution_time_hours\", hue=\"transform_type\")\n\nplt.ylabel(\"Total Execution Time Per Epoch [Hours]\")\nplt.xlabel(None)\nplt.legend(title=\"Transform Type\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: Model Total Execution Time [Hours]\n\n\n\n\n\n\n\nCode\nprint(metrics_grouped.info())\nmetrics_grouped.head()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 9 entries, 5 to 6\nData columns (total 16 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   model                       9 non-null      object \n 1   max_epoch                   9 non-null      int64  \n 2   mean_learning_rate          9 non-null      float64\n 3   mean_execution_time         9 non-null      float64\n 4   max_execution_time          9 non-null      float64\n 5   min_execution_time          9 non-null      float64\n 6   total_execution_time        9 non-null      float64\n 7   mean_training_loss          9 non-null      float64\n 8   max_training_loss           9 non-null      float64\n 9   min_training_loss           9 non-null      float64\n 10  model_type                  9 non-null      object \n 11  transform_type              9 non-null      object \n 12  total_execution_time_hours  9 non-null      float64\n 13  error_min                   9 non-null      float64\n 14  error_max                   9 non-null      float64\n 15  yerr                        9 non-null      object \ndtypes: float64(11), int64(1), object(4)\nmemory usage: 1.5+ KB\nNone\n\n\n\n\n\n\n\n\n\nmodel\nmax_epoch\nmean_learning_rate\nmean_execution_time\nmax_execution_time\nmin_execution_time\ntotal_execution_time\nmean_training_loss\nmax_training_loss\nmin_training_loss\nmodel_type\ntransform_type\ntotal_execution_time_hours\nerror_min\nerror_max\nyerr\n\n\n\n\n5\nv3.model_type_ResNet.transform_NONE.padding_1....\n100\n0.001\n764.092020\n842.218212\n734.275649\n76409.202002\n0.043401\n1.139886\n0.003890\nResNet\nFull Resolution\n21.224778\n29.816371\n78.126192\n(29.81637118577953, 78.12619210958485)\n\n\n8\nv3.model_type_VGNet.transform_NONE.padding_1.s...\n100\n0.001\n297.820536\n310.311009\n294.249442\n29782.053566\n0.030344\n0.442181\n0.003682\nVGNet\nFull Resolution\n8.272793\n3.571094\n12.490473\n(3.571093802452083, 12.490473265647893)\n\n\n4\nv3.model_type_ResNet.transform_48PX_CROP.paddi...\n100\n0.001\n266.090062\n282.700538\n251.574150\n26609.006231\n0.055947\n0.578099\n0.006972\nResNet\nCenter Crop - 48PX\n7.391391\n14.515912\n16.610476\n(14.515912227630622, 16.610475845336907)\n\n\n3\nv3.model_type_ResNet.transform_32PX_CROP.paddi...\n100\n0.001\n168.808576\n176.997332\n165.752459\n16880.857594\n0.073267\n0.579238\n0.012466\nResNet\nCenter Crop - 32PX\n4.689127\n3.056117\n8.188756\n(3.056116886138909, 8.18875616073609)\n\n\n7\nv3.model_type_VGNet.transform_48PX_CROP.paddin...\n100\n0.001\n146.973338\n159.053998\n143.082328\n14697.333795\n0.061036\n0.480737\n0.009605\nVGNet\nCenter Crop - 48PX\n4.082593\n3.891010\n12.080660\n(3.8910101079940773, 12.080660281181338)\n\n\n\n\n\n\n\n\n\n\nCode\n# metrics_grouped.info()\nmetrics_grouped = metrics_grouped.sort_values([\"total_execution_time_hours\"], ascending=False)\nmetrics_grouped = metrics_grouped.reset_index()\nmetrics_grouped = metrics_grouped.rename({\n    \"model_type\": \"Model Type\",\n    \"transform_type\": \"Transform Type\",\n    \"total_execution_time_hours\": \"Execution Time [Hours]\"\n}, axis=\"columns\")\n\nmin_execution_time = metrics_grouped[\"Execution Time [Hours]\"].min()\nmetrics_grouped[\"Time Factor\"] = metrics_grouped[\"Execution Time [Hours]\"] / min_execution_time\n\n\nmetrics_grouped[['Model Type', 'Transform Type', 'Execution Time [Hours]', \"Time Factor\"]]\n\n\n\n\n\n\n\n\n\nModel Type\nTransform Type\nExecution Time [Hours]\nTime Factor\n\n\n\n\n0\nResNet\nFull Resolution\n21.224778\n9.811983\n\n\n1\nVGNet\nFull Resolution\n8.272793\n3.824422\n\n\n2\nResNet\nCenter Crop - 48PX\n7.391391\n3.416959\n\n\n3\nResNet\nCenter Crop - 32PX\n4.689127\n2.167732\n\n\n4\nVGNet\nCenter Crop - 48PX\n4.082593\n1.887338\n\n\n5\nInceptionNet\nFull Resolution\n3.481190\n1.609316\n\n\n6\nInceptionNet\nCenter Crop - 48PX\n3.053294\n1.411505\n\n\n7\nInceptionNet\nCenter Crop - 32PX\n2.806497\n1.297413\n\n\n8\nVGNet\nCenter Crop - 32PX\n2.163149\n1.000000"
  },
  {
    "objectID": "index.html#confusion-matricies",
    "href": "index.html#confusion-matricies",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "4.9 Confusion Matricies",
    "text": "4.9 Confusion Matricies\nAfter each model run a confusion matrix generated by passing the validation data into the model and performing calculations on the predicted output. The following confusion matrix and classification report provide a comprehensive evaluation of the VGNet model’s performance when trained on 32×32 px images.\n\n\nCode\nconfusion_matrix = pd.read_parquet(\"../week_3_sample_confusion_matrix.parquet\")\nconfusion_matrix\n\n\n\n\nTable 12: Sample Confusion Matrix\n\n\n\n\n\n\n\n\n\n\nPredicted 0\nPredicted 1\n\n\n\n\nTrue 0\n25916\n179\n\n\nTrue 1\n490\n17420\n\n\n\n\n\n\n\n\n\n\nIn Table 12 we observe:\n\nTrue Positives (17420): The model correctly identified 17,420 true values.\nTrue Negatives (25916): The model correctly predicted 25,916 false values.\nFalse Positives (179): The model incorrectly predicted 179 instances as true when the labeled value was false.\nFalse Negatives (490): The model incorrectly predicted 490 instances as false when the labeled value was true.\n\nOverall this model correctly predicted {python} round((1 - ((490 + 179) / confusion_matrix.sum().sum())) * 100, 2)% of the validation data, demonstrating strong performance with a low error rate relative to the dataset size. However, without further testing, it is there is a high level of uncertaintity as to whether the model generalizes well beyond the training and validation data."
  },
  {
    "objectID": "index.html#classification-report",
    "href": "index.html#classification-report",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "4.10 Classification Report",
    "text": "4.10 Classification Report\n\n\nCode\nclassification_report = pd.read_parquet(\"../week_3_sample_classification_report.parquet\")\n\nclassification_report = classification_report.rename(\n    {\n        \"precision\": \"Precision\",\n        \"recall\": \"Recall\",\n        \"f1-score\": \"F1 Score\",\n    },\n    axis=\"columns\",\n)\n\nclassification_report = classification_report.rename(\n    {\n        \"Class 0\": \"True\",\n        \"Class 1\": \"False\",\n        \"weighted avg\": \"Average\",\n    },\n    axis=\"index\",\n)\n\nclassification_report.loc[[\"True\", \"False\", \"Average\"]][[\"Precision\", \"Recall\", \"F1 Score\"]]\n\n\n\n\nTable 13: Sample Classification Report\n\n\n\n\n\n\n\n\n\n\nPrecision\nRecall\nF1 Score\n\n\n\n\nTrue\n0.981444\n0.993140\n0.987257\n\n\nFalse\n0.989829\n0.972641\n0.981160\n\n\nAverage\n0.984856\n0.984797\n0.984776\n\n\n\n\n\n\n\n\n\n\nThe classification report in Table 13 highlights the VGNet model’s performance across several key metrics. The model trained on 32×32 px images achieves high accuracy, precision, recall, and F1-scores for both classes. It shows a slight bias toward detecting Class 0 (false), because of the slightly higher recall for this class compared to Class 1. Despite this, the overall performance is excellent. The low numbers of false positives and false negatives suggest that the model is well-fitted to the training data. However, additional testing on unseen data is necessary to confirm the model’s robustness and ensure it is not overfitting."
  },
  {
    "objectID": "index.html#training-loss-results",
    "href": "index.html#training-loss-results",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "4.11 Training Loss Results",
    "text": "4.11 Training Loss Results\nTraining loss is output during each batch using the Cross Entropy loss function and averaged over each epoch. The loss function provides a metric of improvement by the model and a decreasing loss function means that the model is learning the patterns of the training dataset. The loss function is also used to monitor overfitting and the learning rate. An indicator of overfitting is a loss function that is lower validation scores. An indicator of a too high learning rate is oscillating, or increasing in the loss rate after an initial local minimum. In Figure 11 and Figure 12, the training loss is visualized over epoch.\n\n4.11.1 Training Loss by Model\n\n\nCode\nfor model in [\"VGNet\", \"InceptionNet\", \"ResNet\"]:\n    fig, axs = plt.subplots(1, 1, figsize=(8, 2))\n    for df in metrics_df:\n        df = df.set_index(['epoch'])\n        raw_model = df['model'].unique()[0]\n        parts = raw_model.split(\".\")\n        this_model = parts[1].split(\"_\")[-1]\n        if this_model == model:\n            if \"32\" in parts[2]:\n                this_model += \" 32 px Crop\"\n            if \"48\" in parts[2]:\n                this_model += \" 48 px Crop\"\n            if \"NONE\" in parts[2]:\n                this_model += \" Original\"\n\n            df['training_loss'].plot(ax=axs, label=this_model)\n\n\n    plt.ylabel(\"Training Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylim(0, 1)\n    plt.legend()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) VGNet Training Loss Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) InceptionNet Training Loss Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) ResNet Training Loss Comparison\n\n\n\n\n\n\n\nFigure 11: Training Loss Comparison - By Model\n\n\n\n\n\n\n4.11.2 Combined Training Loss\n\n\n\nCode\nfig, axs = plt.subplots(1, 1, figsize=(8, 3.5))\n\nfor df in metrics_df:\n    df = df.set_index(['epoch'])\n    raw_model = df['model'].unique()[0]\n    parts = raw_model.split(\".\")\n    model = parts[1].split(\"_\")[-1]\n    if \"32\" in parts[2]:\n        model += \" 32 px Crop\"\n    if \"48\" in parts[2]:\n        model += \" 48 px Crop\"\n    if \"NONE\" in parts[2]:\n        model += \" Original\"\n\n    df['training_loss'].plot(ax=axs, label=model)\n\nplt.title(\"Training Loss Comparison\")\nplt.ylabel(\"Training Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 12: All Models - Training Loss Comparison\n\n\n\n\n\n\nThe training loss results in Figure 11 and Figure 12 show that all models successfully learned patterns from the test images. Both the VGNet and ResNet models exhibit a significant reduction in training loss, approaching near-zero levels after approximately 30 epochs. Notably, models trained on full-size images achieve lower training losses more quickly compared to those trained on cropped images. The InceptionNet model demonstrates comparatively limited learning capacity, stabilizing at a higher training loss of approximately 0.15 across all configurations.\nThe VGNet and ResNet models have no clear signs of overfitting, maintaining stability throughout the training process. In contrast, the InceptionNet model shows some indications of overfitting, particularly as the training loss stabilizes without further improvement. Based on these observations, the VGNet and ResNet models are likely to deliver higher accuracy in their final configurations compared to the InceptionNet model."
  },
  {
    "objectID": "index.html#submission-procedure",
    "href": "index.html#submission-procedure",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "5.1 Submission Procedure",
    "text": "5.1 Submission Procedure\nThe trained models were evaluated locally using the test image dataset, and the predictions were saved to a .csv file. This file was subsequently uploaded to Kaggle as a dataset, enabling the results to be shared and analyzed within the competition framework. The uploaded dataset was then read into a dataframe and used to generate the final submission.csv file, ensuring compatibility with Kaggle’s submission requirements."
  },
  {
    "objectID": "index.html#kaggle-scores",
    "href": "index.html#kaggle-scores",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "5.2 Kaggle Scores",
    "text": "5.2 Kaggle Scores\nAfter submitting, this Kaggle competition produces a public and a private score based on the scoring metric. The following sections detail the results for all models.\n\n\nCode\nkaggle_results = [\n    {\n        \"model\": \"VGNet\",\n        \"pixels\": 32,\n        \"kaggle_version\": 12,\n        \"kaggle_private_score\": 0.7462,\n        \"kaggle_public_score\": 0.7670,\n        \"training_time\": metrics_grouped[\n            (metrics_grouped[\"Model Type\"] == \"VGNet\")\n            & (metrics_grouped[\"Transform Type\"] == \"Center Crop - 32PX\")\n        ][\"Execution Time [Hours]\"].values[0],\n    },\n    {\n        \"model\": \"InceptionNet\",\n        \"pixels\": 32,\n        \"kaggle_version\": 13,\n        \"kaggle_private_score\": 0.7037,\n        \"kaggle_public_score\": 0.7299,\n        \"training_time\": metrics_grouped[\n            (metrics_grouped[\"Model Type\"] == \"InceptionNet\")\n            & (metrics_grouped[\"Transform Type\"] == \"Center Crop - 32PX\")\n        ][\"Execution Time [Hours]\"].values[0],\n    },\n    {\n        \"model\": \"ResNet\",\n        \"pixels\": 32,\n        \"kaggle_version\": 14,\n        \"kaggle_private_score\": 0.7464,\n        \"kaggle_public_score\": 0.7796,\n        \"training_time\": metrics_grouped[\n            (metrics_grouped[\"Model Type\"] == \"ResNet\")\n            & (metrics_grouped[\"Transform Type\"] == \"Center Crop - 32PX\")\n        ][\"Execution Time [Hours]\"].values[0],\n    },\n    {\n        \"model\": \"VGNet\",\n        \"pixels\": 48,\n        \"kaggle_version\": 15,\n        \"kaggle_private_score\": 0.7734,\n        \"kaggle_public_score\": 0.8124,\n        \"training_time\": metrics_grouped[\n            (metrics_grouped[\"Model Type\"] == \"VGNet\")\n            & (metrics_grouped[\"Transform Type\"] == \"Center Crop - 48PX\")\n        ][\"Execution Time [Hours]\"].values[0],\n    },\n    {\n        \"model\": \"InceptionNet\",\n        \"pixels\": 48,\n        \"kaggle_version\": 16,\n        \"kaggle_private_score\": 0.7205,\n        \"kaggle_public_score\": 0.7432,\n        \"training_time\": metrics_grouped[\n            (metrics_grouped[\"Model Type\"] == \"InceptionNet\")\n            & (metrics_grouped[\"Transform Type\"] == \"Center Crop - 48PX\")\n        ][\"Execution Time [Hours]\"].values[0],\n    },\n    {\n        \"model\": \"ResNet\",\n        \"pixels\": 48,\n        \"kaggle_version\": 17,\n        \"kaggle_private_score\": 0.7523,\n        \"kaggle_public_score\": 0.7551,\n        \"training_time\": metrics_grouped[\n            (metrics_grouped[\"Model Type\"] == \"ResNet\")\n            & (metrics_grouped[\"Transform Type\"] == \"Center Crop - 48PX\")\n        ][\"Execution Time [Hours]\"].values[0],\n    },\n    {\n        \"model\": \"VGNet\",\n        \"pixels\": 96,\n        \"kaggle_version\": 18,\n        \"kaggle_private_score\": 0.7983,\n        \"kaggle_public_score\": 0.8497,\n        \"training_time\": metrics_grouped[\n            (metrics_grouped[\"Model Type\"] == \"VGNet\")\n            & (metrics_grouped[\"Transform Type\"] == \"Full Resolution\")\n        ][\"Execution Time [Hours]\"].values[0],\n    },\n    {\n        \"model\": \"InceptionNet\",\n        \"pixels\": 96,\n        \"kaggle_version\": 19,\n        \"kaggle_private_score\": 0.7688,\n        \"kaggle_public_score\": 0.8163,\n        \"training_time\": metrics_grouped[\n            (metrics_grouped[\"Model Type\"] == \"InceptionNet\")\n            & (metrics_grouped[\"Transform Type\"] == \"Full Resolution\")\n        ][\"Execution Time [Hours]\"].values[0],\n    },\n    {\n        \"model\": \"ResNet\",\n        \"pixels\": 96,\n        \"kaggle_version\": 20,\n        \"kaggle_private_score\": 0.8198,\n        \"kaggle_public_score\": 0.8682,\n        \"training_time\": metrics_grouped[\n            (metrics_grouped[\"Model Type\"] == \"ResNet\")\n            & (metrics_grouped[\"Transform Type\"] == \"Full Resolution\")\n        ][\"Execution Time [Hours]\"].values[0],\n    },\n]\n\n\nkaggle_df = pd.DataFrame(kaggle_results)\n\n# kaggle_df['Accuracy Per Training Hour'] = kaggle_df['training_time'] / kaggle_df[\"kaggle_public_score\"]\nkaggle_df['Accuracy Per Training Hour'] = kaggle_df['kaggle_public_score'] / kaggle_df[\"training_time\"]\n\n# kaggle_df['kaggle_private_score'] *= 100.0\n# kaggle_df['kaggle_public_score'] *= 100.0\n\nkaggle_df = kaggle_df.rename(\n    {\n        \"kaggle_private_score\": \"Kaggle Private Score\",\n        \"kaggle_public_score\": \"Kaggle Public Score\",\n        \"kaggle_version\": \"Kaggle Version\",\n        \"pixels\": \"Image Dims [Px]\",\n        \"model\": \"Model Family\",\n        \"training_time\": \"Training Time [Hours]\",\n    },\n    axis=\"columns\",\n)"
  },
  {
    "objectID": "index.html#results-table-and-proof",
    "href": "index.html#results-table-and-proof",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "5.3 Results Table and Proof",
    "text": "5.3 Results Table and Proof\n\n\n\n\n\n\n\n\n\nFigure 13: Kaggle Test Results Screenshot\n\n\n\n\n\n\n\nCode\nkaggle_df = kaggle_df.sort_index(ascending=False)\nkaggle_df = kaggle_df.reset_index()\nkaggle_df[['Kaggle Version', 'Kaggle Public Score', 'Model Family', \"Image Dims [Px]\"]]\n\n\n\n\nTable 14: Kaggle Test Results\n\n\n\n\n\n\n\n\n\n\nKaggle Version\nKaggle Public Score\nModel Family\nImage Dims [Px]\n\n\n\n\n0\n20\n0.8682\nResNet\n96\n\n\n1\n19\n0.8163\nInceptionNet\n96\n\n\n2\n18\n0.8497\nVGNet\n96\n\n\n3\n17\n0.7551\nResNet\n48\n\n\n4\n16\n0.7432\nInceptionNet\n48\n\n\n5\n15\n0.8124\nVGNet\n48\n\n\n6\n14\n0.7796\nResNet\n32\n\n\n7\n13\n0.7299\nInceptionNet\n32\n\n\n8\n12\n0.7670\nVGNet\n32"
  },
  {
    "objectID": "index.html#kaggle-public-scores",
    "href": "index.html#kaggle-public-scores",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "5.4 Kaggle Public Scores",
    "text": "5.4 Kaggle Public Scores\n\n\n\nCode\nplt.figure(figsize=(8, 3.5))\n\n# Create the barplot\nax = sns.barplot(\n    data=kaggle_df,\n    x=\"Model Family\",\n    y=\"Kaggle Public Score\",\n    hue=\"Image Dims [Px]\",\n    palette=sns.color_palette()[:3]\n);\n\n# Add labels to each bar\nfor container in ax.containers:\n    ax.bar_label(container, fmt=\"%.3f\")  # Format to 1 decimal place\n\n# Move the legend below the x-axis\nplt.legend(\n    title=\"Image Dims [Px]\",\n    loc=\"upper center\", \n    bbox_to_anchor=(0.5, -0.2),  # Position legend below the x-axis\n    ncol=3  # Arrange legend items in 3 columns\n)\n\nplt.xlabel(None)\nplt.ylim((0, 1))\n\n# Adjust layout for better spacing\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 14: All Kaggle Submission Public Stores\n\n\n\n\n\n\n\n\nCode\npublic_df = kaggle_df[['Model Family', \"Image Dims [Px]\", 'Kaggle Public Score', ]]\npublic_df = public_df.sort_values(['Kaggle Public Score'], ascending=False)\npublic_df = public_df.reset_index(drop=True)\npublic_df\n\n\n\n\nTable 15: Kaggle Public Test Results\n\n\n\n\n\n\n\n\n\n\nModel Family\nImage Dims [Px]\nKaggle Public Score\n\n\n\n\n0\nResNet\n96\n0.8682\n\n\n1\nVGNet\n96\n0.8497\n\n\n2\nInceptionNet\n96\n0.8163\n\n\n3\nVGNet\n48\n0.8124\n\n\n4\nResNet\n32\n0.7796\n\n\n5\nVGNet\n32\n0.7670\n\n\n6\nResNet\n48\n0.7551\n\n\n7\nInceptionNet\n48\n0.7432\n\n\n8\nInceptionNet\n32\n0.7299\n\n\n\n\n\n\n\n\n\n\nFigure 14, and Table 15 summarize the Kaggle public scores results for three model families ResNet, VGNet, and InceptionNet, trained on images of varying dimensions (32px, 48px, and 96px). Key observations include:\n\nResNet achieved the highest Kaggle public score (0.8682) when trained on 96px images, demonstrating the strongest performance on larger image sizes.\nVGNet closely followed with a score of 0.8497 on 96px images and maintained competitive performance across all dimensions, ranking second overall with 48px images (0.8124) and third with 32px images (0.7670).\nInceptionNet scored lower than both ResNet and VGNet across all image sizes, peaking at 0.8163 with 96px images and declining to 0.7299 with 32px images.\n\nOverall, larger image dimensions (96px) resulted in better public scores for all models, with ResNet and VGNet consistently outperforming InceptionNet across the board.\n:::"
  },
  {
    "objectID": "index.html#accuracy-vs.-computational-efficiency",
    "href": "index.html#accuracy-vs.-computational-efficiency",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "5.5 Accuracy vs. Computational Efficiency",
    "text": "5.5 Accuracy vs. Computational Efficiency\n\n\n\nCode\nplt.figure(figsize=(8, 3.5))\n\n# Create the barplot\nax = sns.barplot(\n    data=kaggle_df,\n    x=\"Model Family\",\n    y=\"Accuracy Per Training Hour\",\n    hue=\"Image Dims [Px]\",\n    palette=sns.color_palette()[:3]\n);\n\n# Add labels to each bar\nfor container in ax.containers:\n    ax.bar_label(container, fmt=\"%.3f\")  # Format to 1 decimal place\n\n# Move the legend below the x-axis\nplt.legend(\n    title=\"Image Dims [Px]\",\n    loc=\"upper center\", \n    bbox_to_anchor=(0.5, -0.2),  # Position legend below the x-axis\n    ncol=3  # Arrange legend items in 3 columns\n)\n\nplt.xlabel(None)\n\n# Adjust layout for better spacing\nplt.ylim((0, 0.45))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 15: All Kaggle Submission Private Stores\n\n\n\n\n\n\n\n\nCode\nefficency_df = kaggle_df[['Model Family', \"Image Dims [Px]\", 'Accuracy Per Training Hour']]\nefficency_df.loc[:, 'Accuracy Per Training Hour'] = efficency_df['Accuracy Per Training Hour'].round(3)\nefficency_df = efficency_df.sort_values(['Accuracy Per Training Hour'], ascending=False)\nefficency_df = efficency_df.reset_index(drop=True)\nefficency_df\n\n\n\n\nTable 16: Kaggle Accuracy vs. Computational Efficiency\n\n\n\n\n\n\n\n\n\n\nModel Family\nImage Dims [Px]\nAccuracy Per Training Hour\n\n\n\n\n0\nVGNet\n32\n0.355\n\n\n1\nInceptionNet\n32\n0.260\n\n\n2\nInceptionNet\n48\n0.243\n\n\n3\nInceptionNet\n96\n0.234\n\n\n4\nVGNet\n48\n0.199\n\n\n5\nResNet\n32\n0.166\n\n\n6\nVGNet\n96\n0.103\n\n\n7\nResNet\n48\n0.102\n\n\n8\nResNet\n96\n0.041\n\n\n\n\n\n\n\n\n\n\nIn Figure 15 and Table 16 we detail the Accuracy Per Training Hour metric which evaluates model efficiency by dividing the Kaggle public score by the training time. Key findings include:\n\nVGNet with 32px images achieves the highest efficiency (0.355), balancing accuracy with low training time.\nInceptionNet also shows good efficiency at 32px (0.260), but its efficiency decreases with larger image sizes (48px: 0.243, 96px: 0.234).\nResNet models are less efficient, with a significant drop in performance per training hour as image size increases (32px: 0.166, 48px: 0.102, 96px: 0.041).\n\nWhile accuracy is the primary metric, VGNet with 32px images offers the best trade-off between accuracy and training efficiency, making it the most suitable model when training time is a critical factor. In contrast, ResNet shows diminishing returns in efficiency with larger images, indicating higher computational cost relative to its accuracy gains."
  },
  {
    "objectID": "index.html#achievements",
    "href": "index.html#achievements",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "6.1 Achievements",
    "text": "6.1 Achievements\nThe models trained on 96px images consistently outperformed those trained on smaller images in terms of Kaggle public scores, with ResNet 96px achieving the highest score of 0.8682. However, when accounting for the computational cost using the Accuracy Per Training Hour metric, VGNet 32px emerged as the most efficient model, with the highest score of 0.355. This highlights the importance of considering not just accuracy but also the cost of training time when deploying machine learning models in real-world scenarios.\nThe breadth of training included the evaluation of three different architectures (VGNet, InceptionNet, ResNet) across multiple image sizes, revealing critical patterns in model behavior, performance, and computational efficiency. The depth of training involved extensive epochs and rigorous testing, providing a reliable assessment of each model’s ability to generalize to unseen data."
  },
  {
    "objectID": "index.html#future-work",
    "href": "index.html#future-work",
    "title": "CNN Cancer Detection Kaggle Mini-Project",
    "section": "6.2 Future Work",
    "text": "6.2 Future Work\nSeveral areas of future work have the poential to further enhance these cancer image detection accuracy and efficiency:\n\nContinued Hyperparameter Tuning:\n\nStride and Padding: Experimenting with different stride values and padding techniques could help refine the model’s learning ability, especially for smaller image sizes, by adjusting how features are extracted during the convolutional layers.\nLeveraging hyperparamater optimization libraries including Optuna\n\nTraining Rate Scheduling:\n\nImplementing dynamic learning rate scheduling methods (such as learning rate decay, cyclical learning rates, or one-cycle learning) could optimize convergence during training, potentially leading to better generalization and faster convergence.\n\nEarly Stopping:\n\nIntroducing early stopping based on validation loss could prevent overfitting, saving training time while ensuring the model generalizes well to new data.\n\nImage Normalization:\n\nNormalizing the input images using the measured dataset values (mean, standard deviation) could improve model stability and help accelerate convergence by reducing internal covariate shift.\n\nData Augmentation:\n\nExperimenting with data augmentation techniques, such as jittering, rotations, or flipping, could increase model robustness by allowing it to learn from a wider variety of data representations and patterns.\n\nModel Ensembling:\n\nFurther research could explore combining the predictions from multiple models using ensembling techniques (e.g., bagging or boosting) to improve predictive accuracy and reduce model bias.\n\n\nBy addressing these areas, future iterations of this project has the potential produce models that are both more accurate and efficient, with broader applicability across various image classification tasks."
  }
]